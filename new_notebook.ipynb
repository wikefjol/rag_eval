{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import OllamaLLM  # Replaces langchain_community.llms.Ollama\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  # New embedding option\n",
    "from langchain_chroma import Chroma  # Adds Chroma vector storage\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from giskard.rag import KnowledgeBase\n",
    "import pdfplumber\n",
    "import re\n",
    "import textwrap\n",
    "import pandas as pd\n",
    "from uuid import uuid4\n",
    "import torch \n",
    "import os\n",
    "import pickle\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "''\n",
    "AVAILABLE_LLMS = {\n",
    "    \"ChatGPT4o\" :\"gpt-4o\",\n",
    "    \"ChatGPT4o-mini\": \"gpt-4o-mini\",\n",
    "    \"ChatGPT3.5-turbo\": \"gpt-3.5-turbo\",\n",
    "    \"Llama3.2-3b\": \"llama3.2:3b\",\n",
    "}\n",
    "\n",
    "AVAILABLE_EMBS = {\n",
    "    \"ChatGPT4o\" : \"openai\",\n",
    "    \"ChatGPT4o-mini\" : \"openai\",\n",
    "    \"ChatGPT3.5-turbo\" : \"openai\",\n",
    "    \"Llama3.2-3b\": \"sentence-transformers/all-mpnet-base-v2\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_llm(model_name):\n",
    "    \"\"\"\n",
    "    Initializes the LLM based on the provided model name.\n",
    "\n",
    "    Parameters:\n",
    "        model_name (str): The key of the model in AVAILABLE_LLMS (e.g., \"ChatGPT3.5-turbo\", \"Llama3.2-3b\").\n",
    "    \n",
    "    Returns:\n",
    "        llm: The initialized LLM instance.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Ensure `device` is defined\n",
    "\n",
    "    # Check for valid input\n",
    "    if model_name not in AVAILABLE_LLMS.keys():\n",
    "        raise ValueError(f\"Unsupported model: {model_name}. Please choose from: {list(AVAILABLE_LLMS.keys())}\")\n",
    "\n",
    "    # Initialize LLM based on the model name\n",
    "    llm_config = AVAILABLE_LLMS[model_name]\n",
    "    \n",
    "    if model_name.startswith(\"ChatGPT\"):\n",
    "        # For OpenAI GPT models\n",
    "        OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "        llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=llm_config)\n",
    "    elif model_name.startswith(\"Llama\"):\n",
    "        # For Llama models\n",
    "        llm = OllamaLLM(model=llm_config)\n",
    "    else:\n",
    "        raise ValueError(f\"Model configuration for {model_name} is not supported.\")\n",
    "\n",
    "    return llm\n",
    "\n",
    "def init_emb(llm_model_name):\n",
    "    \"\"\"\n",
    "    Initializes the embedding model based on the provided LLM model name.\n",
    "\n",
    "    Parameters:\n",
    "        llm_model_name (str): The name of the LLM model (e.g., \"ChatGPT3.5-turbo\", \"Llama3.2-3b\").\n",
    "    \n",
    "    Returns:\n",
    "        emb_model: The initialized embedding model instance.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Ensure `device` is defined\n",
    "\n",
    "    # Validate input\n",
    "    if llm_model_name not in AVAILABLE_EMBS.keys():\n",
    "        raise ValueError(f\"Unsupported model: {llm_model_name}. Please choose from: {list(AVAILABLE_EMBS.keys())}\")\n",
    "\n",
    "    # Initialize embeddings dynamically based on the model name\n",
    "    emb_model_config = AVAILABLE_EMBS[llm_model_name]\n",
    "\n",
    "    if llm_model_name.startswith(\"ChatGPT\"):\n",
    "        # For OpenAI GPT models\n",
    "        emb_model = OpenAIEmbeddings()\n",
    "    elif llm_model_name.startswith(\"Llama\"):\n",
    "        # For Llama models\n",
    "        model_kwargs = {'device': device}\n",
    "        encode_kwargs = {'normalize_embeddings': False}\n",
    "        emb_model = HuggingFaceEmbeddings(\n",
    "            model_name=emb_model_config,\n",
    "            model_kwargs=model_kwargs,\n",
    "            encode_kwargs=encode_kwargs\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Model configuration for {llm_model_name} is not supported.\")\n",
    "\n",
    "    return emb_model\n",
    "\n",
    "\n",
    "def process_table(table):\n",
    "    \"\"\"\n",
    "    Processes a table by converting it into a string representation.\n",
    "    Cleans each cell using the `clean_text` function.\n",
    "    \"\"\"\n",
    "    if not table:\n",
    "        return \"\"\n",
    "    return \"\\n\".join([\"\\t\".join(clean_text(cell) for cell in row if cell is not None) for row in table])\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans text by:\n",
    "    - Removing unwanted characters (e.g., \\n, \\t, bullet points).\n",
    "    - Removing extra spaces.\n",
    "    - Handles None values gracefully.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Remove non-informative bullet points or markers\n",
    "    text = re.sub(r'[•➢]', '', text)\n",
    "    # Replace newlines, tabs, and carriage returns with a single space\n",
    "    text = re.sub(r'[\\n\\r\\t]', ' ', text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "    \n",
    "\n",
    "def read_files(filepath):\n",
    "    folders = {\n",
    "        \"eu\": os.path.join(filepath, \"eu\"),\n",
    "        \"other_higher\": os.path.join(filepath, \"other/higher-value\"),\n",
    "        \"other_lower\": os.path.join(filepath, \"other/lower-value\"),\n",
    "    }\n",
    "\n",
    "    # Metadata mapping\n",
    "    metadata_mapping = {\n",
    "        \"eu\": {\"type\": \"eu\"},\n",
    "        \"other_higher\": {\"type\": \"other\", \"value\": \"higher\"},\n",
    "        \"other_lower\": {\"type\": \"other\", \"value\": \"lower\"},\n",
    "    }\n",
    "\n",
    "# Initialize a data structure to store extracted content\n",
    "    all_data = []\n",
    "\n",
    "# Loop through folders and process files\n",
    "    for folder_name, folder_path in folders.items():\n",
    "        metadata = metadata_mapping[folder_name]\n",
    "    \n",
    "        for pdf_file in os.listdir(folder_path):\n",
    "            if pdf_file.endswith(\".pdf\"):\n",
    "                file_path = os.path.join(folder_path, pdf_file)\n",
    "                with pdfplumber.open(file_path) as pdf:\n",
    "                    for page in pdf.pages:\n",
    "                        text = page.extract_text()\n",
    "                        tables = page.extract_tables()\n",
    "\n",
    "                        # Append data to the list\n",
    "                        all_data.append({\n",
    "                            \"file_name\": pdf_file,\n",
    "                            \"page_number\": page.page_number,\n",
    "                            \"text\": clean_text(text),  # Clean the text directly\n",
    "                            \"processed_tables\": process_table(tables[0]) if tables else \"\",  # Cleaned tables\n",
    "                            \"metadata\": metadata  # Folder-based metadata\n",
    "                        })\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Remove rows with no text\n",
    "    df = df[df[\"text\"] != \"\"].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def init_vectorstore(df, embedding, persist_directory=\"vectorstore\"):\n",
    "    \"\"\"\n",
    "    Initializes a vector store by embedding and chunking the text data, with support for persistence.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the text data.\n",
    "        embedding: The embedding model to use.\n",
    "        persist_directory (str): Directory to save/load the vector store.\n",
    "\n",
    "    Returns:\n",
    "        vec_store: The initialized or loaded vector store.\n",
    "    \"\"\"\n",
    "    # Check if the vector store already exists\n",
    "    if os.path.exists(persist_directory):\n",
    "        print(f\"Loading vector store from {persist_directory}...\")\n",
    "        vec_store = Chroma(embedding_function=embedding, persist_directory=persist_directory)\n",
    "        print(\"Loaded existing vector store.\")\n",
    "        return vec_store\n",
    "\n",
    "    # Initialize the vector store if it doesn't exist\n",
    "    print(\"Vector store not found. Initializing new vector store...\")\n",
    "\n",
    "    sem_text_splitter = SemanticChunker(embedding, breakpoint_threshold_type=\"gradient\")\n",
    "    chunks = []\n",
    "\n",
    "    # Add tqdm progress bar for iterating over DataFrame rows\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing documents\"):\n",
    "        text = row[\"text\"]\n",
    "        text_chunks = sem_text_splitter.create_documents([text])\n",
    "        unique_id = f\"{row.file_name}_page_{row.page_number}\"\n",
    "\n",
    "        for chunk in text_chunks:\n",
    "            chunk.metadata = {\"id\": unique_id}\n",
    "            chunks.append(chunk)\n",
    "\n",
    "    print(f\"Processed {len(chunks)} chunks. Initializing vector store...\")\n",
    "\n",
    "    # Create a new Chroma vector store\n",
    "    vec_store = Chroma(embedding_function=embedding, persist_directory=persist_directory)\n",
    "    uuids = [str(uuid4()) for _ in range(len(chunks))]\n",
    "    vec_store.add_documents(documents=chunks, ids=uuids)\n",
    "    return vec_store\n",
    "\n",
    "\n",
    "def init_retriever(vec_store, k):\n",
    "    '''\n",
    "    Sets up a retriever on vec_store that when invoked will find the k most similar docs\n",
    "    '''\n",
    "    return vec_store.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "def build_rag_chain(retriever, llm):\n",
    "\n",
    "    template = \"\"\"\n",
    "    Use the following context to answer the question. Only use  information from the context provided.\n",
    "    Do not ask questions.\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Answer: \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    doc_chain = retriever | (lambda docs: docs if docs else None)\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | (lambda docs: \" \".join(doc.page_content for doc in docs) if docs else \"\"),\n",
    "        \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | (lambda output: output.replace(\"\\n\", \" \").strip())\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    combined_chain = RunnableParallel(\n",
    "        {\n",
    "            \"question\": RunnablePassthrough(),\n",
    "            \"answer\": rag_chain,\n",
    "            \"docs\": doc_chain,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return combined_chain\n",
    "\n",
    "def handle_query(rag_chain, query):\n",
    "    return rag_chain.invoke(query)\n",
    "\n",
    "def create_answer_fn(chain):\n",
    "    \"\"\"\n",
    "    Creates an answer_fn for Giskard's evaluation function based on the handle_query function.\n",
    "\n",
    "    Parameters:\n",
    "        chain: The RAG chain to be used for answering questions.\n",
    "\n",
    "    Returns:\n",
    "        A callable `answer_fn` that takes a `question` and `history` and uses the chain to generate answers.\n",
    "    \"\"\"\n",
    "    def answer_fn(question, history=None):\n",
    "        # Call handle_query with the provided chain and question\n",
    "        output = handle_query(chain, question)\n",
    "        return output[\"answer\"]\n",
    "        \n",
    "    return answer_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use another LLM to set up a knowledgebase using giskard.rag.knowledge_base.KnowledgeBase\n",
    "\n",
    "A class to handle the knowledge base and the associated vector store.\n",
    "\n",
    "## Parameters:\n",
    "- **`knowledge_base_df`** (`pd.DataFrame`)  \n",
    "  A dataframe containing the whole knowledge base.\n",
    "\n",
    "- **`columns`** (`Sequence[str]`, *optional*)  \n",
    "  The list of columns from the knowledge base to consider. If not specified, all columns of the knowledge base dataframe will be concatenated to produce a single document.  \n",
    "  **Example**: If your knowledge base consists of FAQ data with columns `\"Q\"` and `\"A\"`, the rows will be formatted into a single document as:  \n",
    "  `Q: [question]\\nA: [answer]`.\n",
    "\n",
    "- **`seed`** (`int`, *optional*)  \n",
    "  The seed to use for random number generation.\n",
    "\n",
    "- **`llm_client`** (`LLMClient`, *optional*)  \n",
    "  The LLM client to use for question generation. If not specified, a default OpenAI client will be used.\n",
    "\n",
    "- **`embedding_model`** (`BaseEmbedding`, *optional*)  \n",
    "  The embedding model to use for the knowledge base. By default, the Giskard default model is used, which is OpenAI `\"text-embedding-ada-002\"`.\n",
    "\n",
    "- **`min_topic_size`** (`int`, *optional*)  \n",
    "  The minimum number of documents to form a topic inside the knowledge base.\n",
    "\n",
    "- **`chunk_size`** (`int`, *default: 2048*)  \n",
    "  The number of documents to embed in a single batch.<>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(AVAILABLE_LLMS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from giskard.rag import KnowledgeBase, generate_testset\n",
    "\n",
    "FILE_PATH = \"./data/raw\"\n",
    "columns = [\"text\", \"file_name\", \"page_number\"]  # Which columns the evaluation should be based on\n",
    "num_most_similar_docs = 5  # How many documents the RAG should take into account when constructing an answer\n",
    "num_questions_in_testset = 30\n",
    "language = \"en\"\n",
    "\n",
    "# We use one LLM to set up the knowledge base - this should preferably be a stronger model than the one we use in the RAG, since this will produce the \"correct answers\".\n",
    "rag_model_name = list(AVAILABLE_LLMS.keys())[2]\n",
    "eval_model_name = list(AVAILABLE_LLMS.keys())[2]\n",
    "\n",
    "print(f\"Selected RAG model: {rag_model_name}\")\n",
    "print(f\"Selected evaluation model: {eval_model_name}\")\n",
    "\n",
    "# Set up the LLMs\n",
    "rag_llm = init_llm(rag_model_name)\n",
    "eval_llm = init_llm(eval_model_name)\n",
    "\n",
    "# Set up the embedding models\n",
    "rag_emb = init_emb(rag_model_name)\n",
    "eval_emb = init_emb(eval_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data and create vector store\n",
    "# This script first checks if preprocessed data and vector store files already exist. \n",
    "# If they do, it loads them to save time. Otherwise, it processes the data and/or initializes \n",
    "# the vector store, saving them for future runs.\n",
    "FILE_PATH = \"./data/raw\"\n",
    "columns = [\"text\", \"file_name\", \"page_number\"]  # Which columns the evaluation should be based on\n",
    "num_most_similar_docs = 5  # How many documents the RAG should take into account when constructing an answer\n",
    "num_questions_in_testset = 30\n",
    "language = \"en\"\n",
    "PROCESSED_DATA_PATH = \"./data/processed/processed_data.pkl\"\n",
    "VECTORSTORE_PATH = \"./chroma_langchain_db\"\n",
    "\n",
    "# Load or process the data\n",
    "if os.path.exists(PROCESSED_DATA_PATH):\n",
    "    print(f\"Loading processed data from {PROCESSED_DATA_PATH}...\")\n",
    "    with open(PROCESSED_DATA_PATH, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    print(f\"Loaded processed data. Number of records: {len(data)}\")\n",
    "else:\n",
    "    print(f\"Processed data file not found. Reading files from {FILE_PATH}...\")\n",
    "    data = read_files(FILE_PATH)\n",
    "    print(f\"Finished processing files. Number of records: {len(data)}\")\n",
    "\n",
    "    print(f\"Saving processed data to {PROCESSED_DATA_PATH}...\")\n",
    "    with open(PROCESSED_DATA_PATH, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(\"Processed data saved successfully.\")\n",
    "\n",
    "# Load or create the vector store\n",
    "if os.path.exists(VECTORSTORE_PATH):\n",
    "    print(f\"Loading vector store from {VECTORSTORE_PATH}...\")\n",
    "    with open(VECTORSTORE_PATH, \"rb\") as f:\n",
    "        vec_store = pickle.load(f)\n",
    "    print(\"Vector store loaded successfully.\")\n",
    "else:\n",
    "    print(\"Vector store file not found. Initializing vector store...\")\n",
    "    vec_store = init_vectorstore(data, eval_emb)\n",
    "    print(\"Vector store initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "num_most_similar_docs = 3\n",
    "columns = [\"text\", \"file_name\", \"page_number\"]\n",
    "\n",
    "# Set up the retriever\n",
    "print(f\"Initializing retriever with {num_most_similar_docs} most similar documents.\")\n",
    "retriever = init_retriever(vec_store, num_most_similar_docs)\n",
    "print(\"Retriever initialized.\")\n",
    "\n",
    "# Build the RAG chain\n",
    "print(\"Building the RAG chain...\")\n",
    "rag_chain = build_rag_chain(retriever, rag_llm)\n",
    "print(\"RAG chain built.\")\n",
    "\n",
    "# Create the answer function\n",
    "print(\"Creating the answer function for the RAG chain...\")\n",
    "answer_fn = create_answer_fn(rag_chain)\n",
    "print(\"Answer function created.\")\n",
    "\n",
    "print(\"Setup complete! Ready to generate test sets or evaluate.\")\n",
    "\n",
    "print(\"Creating the knowledgebase...\")\n",
    "gpt_knowledge_base = KnowledgeBase(\n",
    "    data=data,  # Your DataFrame\n",
    "    columns=columns,  # List of columns to use\n",
    "    seed=42,\n",
    "    min_topic_size=2,\n",
    "    chunk_size=2048\n",
    ")\n",
    "\n",
    "num_questions_in_testset = 10\n",
    "language = \"en\"\n",
    "\n",
    "testset = generate_testset(\n",
    "    knowledge_base=gpt_knowledge_base,\n",
    "    num_questions=num_questions_in_testset,  # Specify the number of questions\n",
    "    language=language,  # Language for questions\n",
    "    agent_description=\"This LLM assists with queries using a knowledge base.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testset Generation\n",
    "\n",
    "The `giskard.rag.generate_testset` function generates a test set from a knowledge base.\n",
    "\n",
    "```python\n",
    "giskard.rag.generate_testset(\n",
    "    knowledge_base: KnowledgeBase, \n",
    "    num_questions: int = 120, \n",
    "    question_generators: QuestionGenerator | Sequence[QuestionGenerator] = None, \n",
    "    language: str | None = 'en', \n",
    "    agent_description: str | None = 'This agent is a chatbot that answers questions from users.'\n",
    ") -> QATestset\n",
    "```\n",
    "\n",
    "## Parameters:\n",
    "- **`knowledge_base`** (`KnowledgeBase`):  \n",
    "  The knowledge base to generate questions from.\n",
    "\n",
    "- **`num_questions`** (`int`):  \n",
    "  The number of questions to generate. By default, 120.\n",
    "\n",
    "- **`question_generators`** (`Union[BaseQuestionModifier, Sequence[BaseQuestionModifier]]`):  \n",
    "  Question generators to use for question generation. If multiple generators are specified, `num_questions` will be generated with each generator.  \n",
    "  If not specified, all available question generators will be used.\n",
    "\n",
    "- **`language`** (`str`, optional):  \n",
    "  The language to use for question generation. The default is `\"en\"` for English.\n",
    "\n",
    "- **`agent_description`** (`str`, optional):  \n",
    "  Description of the agent to be evaluated. This will be used in the prompt for question generation to get more fitting questions.\n",
    "\n",
    "## Returns:\n",
    "- The generated test set.\n",
    "\n",
    "### Return Type:\n",
    "- **`QATestset`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from giskard.rag import evaluate as gk_eval\n",
    "\n",
    "report = gk_eval(answer_fn, testset=testset, knowledge_base=gpt_knowledge_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
